{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.thuatngumarketing.com/wp-content/uploads/2017/12/NLP.png.pagespeed.ce_.1YNuw_5dJH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis\n",
    ">* Amazon Comprehend Medical is a service that uses NLP to extract disease conditions, medications and treatment outcomes from patient notes, clinical trial reports and other electronic health records.\n",
    ">* Organizations can determine what customers are saying about a service or product by identifying and extracting information in sources like social media.\n",
    ">* An inventor at IBM developed a cognitive assistant that works like a personalized search engine by learning all about you and then remind you of a name, a song, or anything you can’t remember the moment you need it to.\n",
    ">* Companies like Yahoo and Google filter and classify your emails with NLP by analyzing text in emails that flow through their servers and stopping spam before they even enter your inbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1120/1*CGHaWd635jtRa47n4nhsiQ.png)\n",
    "Number of publications containing the sentence “natural language processing” in PubMed in the period 1978–2018. As of 2018, PubMed comprised more than 29 million citations for biomedical literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete html\n",
    "* Delete link\n",
    "* Delete punctuation \n",
    "* Delete emotion icon\n",
    "* Delete numbers\n",
    "* Delete space\n",
    "* Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Suppose we have a sentence as “Can I eat the Pizza”. Try to apply one hot ending i.e converting the categories into numerical labels. \n",
    ">1. Firstly, convert the text to lower and then sort the words in ascending form i.e A-Z. Now we’ll have “can, eat, i, pizza, the”.\n",
    "2. Give a numerical label as we can see can is at 0th position and eat is at 1 same way, assign the values like can:0, i:2, eat:1, the:4, pizza:3.\n",
    "3. Transform to binary vectors."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[1. 0. 0. 0. 0.] #can\n",
    " [0. 0. 1. 0. 0.] #i\n",
    " [0. 1. 0. 0. 0.] #eat\n",
    " [0. 0. 0. 0. 1.] #the\n",
    " [0. 0. 0. 1. 0.]] #pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables are basically the fixed value number on the basis of some qualitative properties. Such as Sex of an individual as it can be either male or female or trans. Weather is also one example as it can be sunny, cloudy, or rainy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to follow:**\n",
    "1. Convert Text to lower case\n",
    "2. Tokenize the text\n",
    "3. Get unique words\n",
    "4. Sort the word list\n",
    "5. Get the integer/position of the words\n",
    "6. Create a vector of each word by marking its position as 1 and rest as 0\n",
    "7. Create a matrix of the found vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "\n",
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms. \n",
    "\n",
    "The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    ">1. A vocabulary of known words.\n",
    "2. A measure of the presence of known words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "*“It was the best of times”*\n",
    "\n",
    "*“It was the worst of times”*\n",
    "\n",
    "*“It was the age of wisdom”*\n",
    "\n",
    "*“It was the age of foolishness”*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We treat each sentence as a separate document and we make a list of all words from all the four documents excluding the punctuation. We get, *‘It’, ‘was’, ‘the’, ‘best’, ‘of’, ‘times’, ‘worst’, ‘age’, ‘wisdom’, ‘foolishness’*.\n",
    "\n",
    "The next step is the create vectors. Vectors convert text that can be used by the machine learning algorithm.\n",
    "\n",
    "We take the first document — “It was the best of times” and we check the frequency of words from the 10 unique words.\n",
    "\n",
    "“it” = 1\n",
    "\n",
    "“was” = 1\n",
    "\n",
    "“the” = 1\n",
    "\n",
    "“best” = 1\n",
    "\n",
    "“of” = 1\n",
    "\n",
    "“times” = 1\n",
    "\n",
    "“worst” = 0\n",
    "\n",
    "“age” = 0\n",
    "\n",
    "“wisdom” = 0\n",
    "\n",
    "“foolishness” = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the documents will be:\n",
    "\n",
    "“It was the best of times” = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "“It was the worst of times” = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "“It was the age of wisdom” = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\n",
    "“It was the age of foolishness” = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, each word or token is called a “gram”. Creating a vocabulary of two-word pairs is called a bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF\n",
    "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "TF-IDF (term frequency-inverse document frequency) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for a word in a document is calculated by multiplying two different metrics:\n",
    ">* The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n",
    ">* The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n",
    ">* So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put it in more formal mathematical terms, the TF-IDF score for the word t in the document d from the document set D is calculated as follows:\n",
    "\n",
    "$$ tf-idf(t, d, D)  = tf(t, d) \\dot idf(t, D)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$ tf(t, d) = \\log(1 + freq(t, d)) $$\n",
    "$$ idf(t, D) = \\log \\left( \\dfrac{N}{count(d \\in D: t \\in d)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "Consider the following similar sentences: Have a good day and Have a great day. They hardly have different meaning. If we construct an exhaustive vocabulary (let’s call it V), it would have V = {Have, a, good, great, day}.\n",
    "\n",
    "Have = [1,0,0,0,0]`; a=[0,1,0,0,0]` ; good=[0,0,1,0,0]` ; great=[0,0,0,1,0]` ; day=[0,0,0,0,1]` (` represents transpose)\n",
    "\n",
    "From this representation, 'good' and ‘great’ are as different as ‘day’ and ‘have’, which is not true. Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec was developed at Google by Tomas Mikolov, et al. and uses Neural Networks to learn word embeddings. The beauty with word2vec is that the vectors are learned by understanding the context in which words appear. The result is vectors in which words with similar meanings end up with a similar numerical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in a regular one-hot encoded Vector, all words end up with the same distance between each other, even though their meanings are completely different. In other words, information is lost in the encoding.\n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/b3c56245-db43-48ab-b652-9ba03f4d9900.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With word embeddings methods such as Word2Vec, the resulting vector does a better job of maintaining context. For instance, cats and dogs are more similar than fish and sharks. This extra information makes a big difference in your machine learning algorithms.\n",
    "![](https://i1.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/8cbfc874-3ba3-46c8-ab68-2711812ecbf1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is composed of two different learning models, CBOW and Skip-Gram. CBOW stands for Continuous Bag of Words model. \n",
    "\n",
    "Continuous Bag of Words (CBOW) model can be thought of as learning word embeddings by training a model to predict a word given its context.\n",
    "\n",
    "Skip-Gram Model is the opposite, learning word embeddings by training a model to predict context given a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i0.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/7938152f-71c8-4f28-9c25-06735e6e2b67.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip-gram**\n",
    "\n",
    "Window Size defines how many words before and after the target word will be used as context, typically a Window Size is 5. \n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/a8066c1d-c532-4549-bb24-19dfea5eb178_med.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a window size of 2 the input pairs for training on w(4) royal would be:\n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/training_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sense, a vanilla explanation for context is that context is words that usually appear with one another. In a trained skip-gram model, by inputting the word “Royal”, the context would be predicted to be the words “The” and “King” given by the model’s output layer which contains a probability distribution of each word in the vocabulary when provided a word.\n",
    "![](https://i1.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/56060b2d-41f6-4788-9ae7-bba23fa00f0e_med.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Skip-Gram, the input layer consists of your vocabulary a  (R x V) vector in which V=Vocabulary Size and R is the number of training samples. Each word in your vocabulary is represented by a one-hot encoded vector. This input vector then goes through a hidden layer vector (V x E) in which E = Embedding Dimensions or Features you are trying to learn. The output layer is a vector (R x V). In the output layer, the vector holds a probability for each word in your vocabulary for the given word input. Softmax is applied to this layer. The word embedding is the Hidden-Layer a vector (V x E). \n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/word2vec_weight_matrix_lookup_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/matrix_mult_w_one_hot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/linear-relationships.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
